{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Prepared for Gabor's Data Analysis\n",
    "\n",
    "### Data Analysis for Business, Economics, and Policy\n",
    "by Gabor Bekes and Gabor Kezdi\n",
    " \n",
    "Cambridge University Press 2021\n",
    "\n",
    "**[gabors-data-analysis.com ](https://gabors-data-analysis.com/)**\n",
    "\n",
    " License: Free to share, modify and use for educational purposes. \n",
    " Not to be used for commercial purposes.\n",
    "\n",
    "### Chapter 16\n",
    "\n",
    "**CH16A Predicting apartment prices with random forest and gradient boosintg machines**\n",
    "\n",
    "using the airbnb dataset\n",
    "\n",
    "version 1.1 2023-02-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from patsy import dmatrices\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from mizani.formatters import percent_format\n",
    "# from plotnine import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.inspection import plot_partial_dependence\n",
    "from sklearn.inspection import partial_dependence\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART I\n",
    "### Loading and preparing data \n",
    "----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/peterduronelly/DA3-Coding-Examples/main/data/airbnb_london_workfile_adj_book.csv'\n",
    "data = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA, sample definition and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We focus on normal apartments, n<8\n",
    "data = data[data.n_accommodates < 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy a variable - purpose later, see at variable importance\n",
    "data['n_accommodates_copy'] = data['n_accommodates']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***numerical variables***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# too long to display and read\n",
    "data.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.price.describe(percentiles = [0.01, 0.1, 0.25, 0.5, 0.75, 0.9, 0.99]).map('{:,.1f}'.format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***categorical variables***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.f_room_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.f_property_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.f_number_of_reviews.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***split train and test***\n",
    "- train is where we do it all, incl CV\n",
    "\n",
    "- first pick a smaller than usual training set so that models run faster and check if works\n",
    "- if works, start anew without these two lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_train, data_holdout = train_test_split( data, train_size=0.7, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_train.shape, data_holdout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic variables inc neighbourhood\n",
    "basic_vars = [\n",
    "    \"n_accommodates\",\n",
    "    \"n_beds\",\n",
    "    \"n_days_since\",\n",
    "    \"f_property_type\",\n",
    "    \"f_room_type\",\n",
    "    \"f_bathroom\",\n",
    "    \"f_cancellation_policy\",\n",
    "    \"f_bed_type\",\n",
    "    \"f_neighbourhood_cleansed\",\n",
    "]\n",
    "\n",
    "# reviews\n",
    "reviews = [\n",
    "    \"n_number_of_reviews\",\n",
    "    \"flag_n_number_of_reviews\",\n",
    "    \"n_review_scores_rating\",\n",
    "    \"flag_review_scores_rating\",\n",
    "]\n",
    "\n",
    "# dummy variables\n",
    "amenities = [col for col in data if col.startswith(\"d_\")]\n",
    "\n",
    "# interactions for the LASSO\n",
    "# from ch14\n",
    "X1 = [\n",
    "    \"n_accommodates:f_property_type\",\n",
    "    \"f_room_type:f_property_type\",\n",
    "    \"f_room_type:d_familykidfriendly\",\n",
    "    \"d_airconditioning:f_property_type\",\n",
    "    \"d_cats:f_property_type\",\n",
    "    \"d_dogs:f_property_type\",\n",
    "]\n",
    "# with boroughs\n",
    "X2 = [\n",
    "    \"f_property_type:f_neighbourhood_cleansed\",\n",
    "    \"f_room_type:f_neighbourhood_cleansed\",\n",
    "    \"n_accommodates:f_neighbourhood_cleansed\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors_1 = basic_vars\n",
    "predictors_2 = basic_vars + reviews + amenities\n",
    "predictors_E = basic_vars + reviews + amenities + X1 + X2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PART II\n",
    "### RANDOM FORESTS \n",
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data preparation we are using the [patsy](https://patsy.readthedocs.io/en/latest/overview.html) package (not [this](https://en.wikipedia.org/wiki/Patsy_%28Monty_Python%29) Patsy, bur almost). `patsy` is a Python package for describing statistical models (especially linear models, or models that have a linear component) and building design matrices. It is closely inspired by and compatible with the formula mini-language used in R and S."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, X = dmatrices(\"price ~ \" + \" + \".join(predictors_2), data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dmatrices()` constructs two design matrices given a formula_like and data. By convention, the first matrix is the “outcome” or “y” data, and the second is the “predictor” or “x” data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is X?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.asarray(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('The theoretical recommended number of variables: {:.2f}.'.format(math.sqrt(len(X.design_info.column_names))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a two-dimensional object\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# using ravel() we flatten it to a one-dimensional data object. \n",
    "y.ravel().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor(random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_grid = {\"max_features\": [6, 8, 10, 12], \"min_samples_leaf\": [5, 10, 15]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random = GridSearchCV(\n",
    "    rfr,\n",
    "    tune_grid,\n",
    "    cv=5,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    verbose=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GridsearchCV()` is an exhaustive search over specified parameter values for an estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "rf_model = rf_random.fit(X, y.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Cross-validated results are saved in the grid search object's `cv_results_` attribute. Note that *RMSE* is displayed as a negative number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rf_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_rf_model_cv_results = pd.DataFrame(rf_model.cv_results_)[[\n",
    "    'param_max_features', 'param_min_samples_leaf', 'mean_test_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rf_model_cv_results.columns = ['max features', 'min node size', 'RMSE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_rf_model_cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_rf_model_cv_results.pivot(\n",
    "    index = 'max features', \n",
    "    columns = 'min node size', \n",
    "    values = 'RMSE').round(2)*-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rf_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rf_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rf_model.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PART III\n",
    "### MODEL DIAGNOSTICS \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### feature importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***individual***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_var_imp = pd.DataFrame(\n",
    "    rf_model.best_estimator_.feature_importances_, \n",
    "    X.design_info.column_names)\\\n",
    "    .reset_index()\\\n",
    "    .rename({\"index\": \"variable\", 0: \"imp\"}, axis=1)\\\n",
    "    .sort_values(by=[\"imp\"], ascending=False)\\\n",
    "    .reset_index(drop = True)\n",
    "\n",
    "df_var_imp['cumulative_imp'] = df_var_imp['imp'].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_var_imp.style.format({\n",
    "    'imp': lambda x: f'{x:,.1%}',\n",
    "    'cumulative_imp': lambda x: f'{x:,.1%}'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "Plotting var imp per se results in a nasty chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_var_imp\\\n",
    "    .sort_values(by = 'imp')\\\n",
    "    .plot(kind = 'barh', \n",
    "          x = 'variable', y = 'imp', \n",
    "          figsize = (10,10), grid = True, \n",
    "          title = 'Random forest model highest feature importances', \n",
    "          xlabel = 'variables', legend = False\n",
    "         );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we only care for variables with an importance of more than 1 pct\n",
    "cutoff = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_var_imp[df_var_imp.imp > cutoff]\\\n",
    "    .sort_values(by = 'imp')\\\n",
    "    .plot(kind = 'barh', \n",
    "          x = 'variable', y = 'imp', \n",
    "          figsize = (10,6), grid = True, \n",
    "          title = 'Random forest model highest feature importances', \n",
    "          xlabel = 'variables', legend = False\n",
    "         );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***grouped variable importance - keep binaries created off factors together***\n",
    "\n",
    "For this, you need to create an `sklearn` Pipeline inclduing `OneHotEncoding` (before, encoding was done by patsy's `dmatrices`). This way permutation_importance can calculate factor variables' importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [col for col in predictors_2 if col.startswith(\"f_\")]\n",
    "numerical_columns = [col for col in predictors_2 if col not in categorical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_encoder = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    (\"cat\", categorical_encoder, categorical_columns),\n",
    "    (\"num\", \"passthrough\", numerical_columns)])\n",
    "\n",
    "rf = Pipeline(\n",
    "    [(\"preprocess\", preprocessing), \n",
    "     (\"regressor\", rf_model.best_estimator_)] # put best model to pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(data_train[predictors_2],data_train.price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Permutation feature importance overcomes limitations of the impurity-based feature importance: they do not have a bias toward high-cardinality features and can be computed on a left-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "result = permutation_importance(\n",
    "    rf,\n",
    "    data_holdout[predictors_2],\n",
    "    data_holdout.price,\n",
    "    n_repeats=10,\n",
    "    random_state=45,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "        result.importances_mean,\n",
    "        data_train[predictors_2].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = [\n",
    "    \"f_bed_type\",\n",
    "    \"f_property_type\",\n",
    "    \"f_room_type\",\n",
    "    \"f_bathroom\",\n",
    "    \"n_days_since\",\n",
    "    \"n_accommodates\",\n",
    "    \"n_beds\",\n",
    "    \"f_neighbourhood_cleansed\",\n",
    "    \"f_cancellation_policy\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_grouped_var_imp = pd.DataFrame(\n",
    "        result.importances_mean,\n",
    "        data_train[predictors_2].columns)\\\n",
    "    .loc[grouped]\\\n",
    "    .sort_values(by = 0, ascending = False)\\\n",
    "    .reset_index()\\\n",
    "    .rename({'index': 'variable', 0: 'imp'}, axis = 1)\n",
    "df_grouped_var_imp['cumulative_imp'] = df_grouped_var_imp.imp.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_grouped_var_imp.style.format({\n",
    "    'imp': lambda x: f'{x:,.1%}',\n",
    "    'cumulative_imp': lambda x: f'{x:,.1%}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped_var_imp\\\n",
    "    .sort_values(by = 'imp')\\\n",
    "    .plot(kind = 'barh', \n",
    "          x = 'variable', y = 'imp', \n",
    "          figsize = (10,6), grid = True, \n",
    "          title = 'Random forest model grouped feature importances', \n",
    "          xlabel = 'variables', legend = False\n",
    "         );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_clean_varimp = pd.DataFrame(\n",
    "        result.importances_mean,\n",
    "        data_train[predictors_2].columns)\\\n",
    "    .sort_values(by = 0, ascending = False)\\\n",
    "    .reset_index()\\\n",
    "    .rename({'index': 'variable', 0: 'imp'}, axis = 1)\n",
    "\n",
    "df_clean_varimp['cumulative_imp'] = df_var_imp['imp'].cumsum()\n",
    "df_clean_varimp[df_clean_varimp.cumulative_imp < 0.91]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_clean_varimp.iloc[0:10]\\\n",
    "    .sort_values(by = 'imp')\\\n",
    "    .plot(kind = 'barh', \n",
    "          x = 'variable', y = 'imp', \n",
    "          figsize = (10,6), grid = True, \n",
    "          title = 'Random forest model top 10 feature importances with grouped variables', \n",
    "          xlabel = 'variables', legend = False\n",
    "         );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Dependence Plots \n",
    "-------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Note: easy way, sklearn has plot_partial_dependence function we do this on holdout set!\n",
    "# Also, note that we run it not on the rf_model but on the rf pipeline to manage OneHot_Encoding on the fly.\n",
    "\n",
    "plot_partial_dependence(\n",
    "    rf,\n",
    "    data_holdout[predictors_2],\n",
    "    [\"n_accommodates\"],\n",
    "    feature_names=data_holdout[predictors_2].columns,\n",
    "    line_kw={\"marker\": \"o\", \"color\": 'k'},\n",
    ")\n",
    "\n",
    "plt.grid()\n",
    "plt.ylim(70, 130)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do our pdp plots using Pandas built-in plot method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accomodates_pdp = partial_dependence(\n",
    "    rf, data_holdout[predictors_2], [\"n_accommodates\"], kind=\"average\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accomodates_pdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to access the elements of this complex data structure\n",
    "type(accomodates_pdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    {'number of accomodates': accomodates_pdp['values'][0], \n",
    "     'average price': accomodates_pdp['average'][0]}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    {'number of accomodates': accomodates_pdp['values'][0], \n",
    "     'average price': accomodates_pdp['average'][0]}\n",
    "    ).sort_values(by = 'average price').plot(\n",
    "    kind = 'line', color = 'k', marker = 'o', markersize = 10, linewidth = 0,\n",
    "    figsize = (10,6), legend = False, grid = True,\n",
    "    x = 'number of accomodates', y = 'average price', ylim = (0, 140), \n",
    "    title = 'Partial dependence plot: number of accomodates'\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roomtype_pdp = partial_dependence(\n",
    "    rf, data_holdout[predictors_2], [\"f_room_type\"], kind=\"average\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    {'room type': roomtype_pdp['values'][0], \n",
    "     'average price': roomtype_pdp['average'][0]}\n",
    "    ).sort_values(by = 'average price').plot(\n",
    "    kind = 'line', color = 'k', marker = 'o', markersize = 10, linewidth = 0,\n",
    "    figsize = (10,6), legend = False, grid = True,\n",
    "    x = 'room type', y = 'average price', ylim = (0, 140), \n",
    "    title = 'Partial dependence plot: room type'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsample performance: RMSE / mean(y) \n",
    "---------------------------------------\n",
    "NOTE  we do this on the holdout set, using the encoding pipeline `rf` again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_holdout_w_prediction = data_holdout.assign(\n",
    "    predicted_price=rf.predict(data_holdout[predictors_2])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Creating tables of heterogeneity by various grouping factors***\n",
    "- apartman size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how we start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_holdout_w_prediction['is_low_size'] = data_holdout_w_prediction.n_accommodates.map(lambda x: 'small apt' if x < 3 else 'large apt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_holdout_w_prediction.groupby('is_low_size').apply(lambda x: mean_squared_error(x.predicted_price, x.price, squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it in a function with additional columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rmse(groupby_obj):\n",
    "    return (\n",
    "        groupby_obj.apply(\n",
    "            lambda x: mean_squared_error(x.predicted_price, x.price, squared=False),\n",
    "        )\n",
    "        .to_frame(name=\"rmse\")\n",
    "        .assign(mean_price=groupby_obj.apply(lambda x: np.mean(x.price)).values)\n",
    "        .assign(rmse_norm=lambda x: x.rmse / x.mean_price).round(2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cheaper or more expensive flats - not used in book\n",
    "grouped_object = data_holdout_w_prediction.assign(\n",
    "    is_low_size=lambda x: np.where(x.n_accommodates <= 3, \"small apt\", \"large apt\")\n",
    ").groupby(\"is_low_size\")\n",
    "accom_subset = calculate_rmse(grouped_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accom_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- fancy neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grouped_object = data_holdout_w_prediction.loc[\n",
    "    lambda x: x.f_neighbourhood_cleansed.isin(\n",
    "        [\n",
    "            \"Westminster\",\n",
    "            \"Camden\",\n",
    "            \"Kensington and Chelsea\",\n",
    "            \"Tower Hamlets\",\n",
    "            \"Hackney\",\n",
    "            \"Newham\",\n",
    "        ]\n",
    "    )\n",
    "].groupby(\"f_neighbourhood_cleansed\")\n",
    "neightbourhood_subset = calculate_rmse(grouped_object)\n",
    "\n",
    "neightbourhood_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- property type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grouped_object = data_holdout_w_prediction.loc[\n",
    "    lambda x: x.f_property_type.isin([\"Apartment\", \"House\"])\n",
    "].groupby(\"f_property_type\")\n",
    "proptype_subset = calculate_rmse(grouped_object)\n",
    "\n",
    "proptype_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_holdout = pd.DataFrame(\n",
    "    [\n",
    "        mean_squared_error(\n",
    "            data_holdout_w_prediction.price,\n",
    "            data_holdout_w_prediction.predicted_price,\n",
    "            squared=False,\n",
    "        ),\n",
    "        data_holdout_w_prediction.price.mean(),\n",
    "    ],\n",
    "    index=[\"rmse\", \"mean_price\"],\n",
    ").T.assign(rmse_norm=lambda x: x.rmse / x.mean_price).round(2)\n",
    "all_holdout.index = [\"Total\"]\n",
    "\n",
    "all_holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_rows = pd.DataFrame(\n",
    "    None,\n",
    "    index=[\"Apartment size\", \"Type\", \"Borough\", \"------\"],\n",
    "    columns=[\"rmse\", \"mean_price\", \"rmse_norm\"],\n",
    ").fillna(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance across subsamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.concat(\n",
    "    [\n",
    "        type_rows.iloc[[0]],\n",
    "        accom_subset,\n",
    "        type_rows.iloc[[1]],\n",
    "        proptype_subset,\n",
    "        type_rows.iloc[[2]],\n",
    "        neightbourhood_subset,\n",
    "        type_rows.iloc[[3]],\n",
    "        all_holdout,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART IV\n",
    "### HORSERACE: compare with other models \n",
    "-----------------------------------------------\n",
    "**NOTE: this part of the code is not finished**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ***OLS with dummies for area***\n",
    "\n",
    " using model B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y, X = dmatrices(\"price ~ \" + \" + \".join(predictors_2), data_train)\n",
    "\n",
    "ols_model = LinearRegression().fit(X,y)\n",
    "\n",
    "#y_test, X_test = dmatrices(\"price ~ \" + \" + \".join(predictors_2), data_holdout)\n",
    "\n",
    "y_hat = ols_model.predict(X)\n",
    "\n",
    "ols_rmse = mean_squared_error(y,y_hat,squared=False)\n",
    "ols_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_model_coeffs_df = pd.DataFrame(\n",
    "    ols_model.coef_.tolist()[0],\n",
    "    index=X.design_info.column_names,\n",
    "    columns=[\"ols_coefficient\"],\n",
    ").assign(ols_coefficient=lambda x: x.ols_coefficient.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ols_model_coeffs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  LASSO\n",
    "\n",
    "using extended model w interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter `l1_ratio7` corresponds to *alpha* in the `glmnet` R package while `alpha` corresponds to the *lambda* parameter in glmnet. Specifically, l1_ratio = 1 is the lasso penalty. Currently, l1_ratio <= 0.01 is not reliable, unless you supply your own sequence of alpha.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model =  ElasticNet(l1_ratio = 1,normalize=True,fit_intercept=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model_cv = GridSearchCV(\n",
    "    lasso_model,\n",
    "    {\"alpha\":[i/100 for i in range(1, 26, 1)]},\n",
    "    cv=5,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    verbose=3,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, X = dmatrices(\"price ~ \" + \" + \".join(predictors_E), data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "lasso_model_cv.fit(X, y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    lasso_model_cv.best_estimator_.coef_.tolist(),\n",
    "    index=X.design_info.column_names,\n",
    "    columns=[\"lasso_coefficient\"],\n",
    ").assign(lasso_coefficient=lambda x: x.lasso_coefficient.round(3)).loc[\n",
    "    lambda x: x.lasso_coefficient != 0\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model_cv.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "lasso_rmse = pd.DataFrame(lasso_model_cv.cv_results_).loc[\n",
    "    lambda x: x.param_alpha == lasso_model_cv.best_estimator_.alpha\n",
    "].mean_test_score.values[0] * -1\n",
    "lasso_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. ***CART model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, X = dmatrices(\"price ~ \" + \" + \".join(predictors_2), data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cart_model = DecisionTreeRegressor(random_state=2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get potential ccp_alpha parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = cart_model.cost_complexity_pruning_path(X, y.ravel())\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccp_alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ccp_alphas.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply random search to select a \"best\" alpha, default is 10 iterations\n",
    "`RandomizedSearchCV` does not calculate all potential alphas, just a random 10-element subset of the many potential alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cart_model_cv = RandomizedSearchCV(\n",
    "    cart_model,\n",
    "    {\"ccp_alpha\":ccp_alphas},\n",
    "    cv=5,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    verbose=3,\n",
    ")\n",
    "cart_model_cv.fit(X,y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cart_rmse = pd.DataFrame(cart_model_cv.cv_results_).loc[\n",
    "    lambda x: x.param_ccp_alpha == cart_model_cv.best_estimator_.ccp_alpha\n",
    "].mean_test_score.values[0] * -1\n",
    "cart_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. GBM\n",
    "\n",
    "**NOTE:** With complex grid search run for a **very long time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = GradientBoostingRegressor(learning_rate=0.1, min_samples_split=20, max_features = 10, n_estimators = 50)\n",
    "\n",
    "tune_grid = {\"n_estimators\": [200, 300], \"max_depth\": [5, 10]}\n",
    "\n",
    "gbm_model_cv = GridSearchCV(\n",
    "    gbm,\n",
    "    tune_grid,\n",
    "    cv=5,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    verbose=10,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [col for col in predictors_2 if col.startswith(\"f_\")]\n",
    "numerical_columns = [col for col in predictors_2 if col not in categorical_columns]\n",
    "\n",
    "categorical_encoder = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "preprocessing = ColumnTransformer(\n",
    "    [\n",
    "        (\"cat\", categorical_encoder, categorical_columns),\n",
    "        (\"num\", \"passthrough\", numerical_columns),\n",
    "    ]\n",
    ")\n",
    "\n",
    "gbm_pipe = Pipeline(\n",
    "    [(\"preprocess\", preprocessing), (\"regressor\", gbm_model_cv)], verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "gbm_pipe.fit(data_train[predictors_2],data_train.price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_model_cv.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_model_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_rmse = gbm_model_cv.best_score_*-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gbm_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing model results on CV RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({'model': ['OLS', 'LASSO', 'CART', 'random forest', 'GBM'],\n",
    "              'CV RMSE': [ols_rmse, lasso_rmse, cart_rmse, all_holdout.rmse[0], gbm_rmse]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***GBM with complex tuning parameter set: takes forever to run***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram-based Gradient Boosting Regression Tree. It is experimental so we need to enable experimental features first. This implementation is inspired by [LightGBM](https://github.com/Microsoft/LightGBM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_broad = HistGradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_grid = {\n",
    "    \"max_iter\": [50, 100, 200],\n",
    "    \"max_depth\": [1, 5, 10],\n",
    "    \"learning_rate\": [0.1, 0.15, 0.2],\n",
    "    \"min_samples_leaf\": [5, 10, 20, 30],\n",
    "}\n",
    "\n",
    "gbm_model_cv_broad = GridSearchCV(\n",
    "    gbm_broad,\n",
    "    tune_grid,\n",
    "    cv=5,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    verbose=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [col for col in predictors_2 if col.startswith(\"f_\")]\n",
    "numerical_columns = [col for col in predictors_2 if col not in categorical_columns]\n",
    "\n",
    "categorical_encoder = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "preprocessing = ColumnTransformer(\n",
    "    [\n",
    "        (\"cat\", categorical_encoder, categorical_columns),\n",
    "        (\"num\", \"passthrough\", numerical_columns),\n",
    "    ]\n",
    ")\n",
    "\n",
    "gbm_pipe_broad = Pipeline(\n",
    "    [(\"preprocess\", preprocessing), (\"regressor\", gbm_model_cv_broad)], verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "gbm_pipe_broad.fit(data_train[predictors_2],data_train.price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gbm_model_cv_broad.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Broad GBM RMSE is: {:.4f}.'.format(gbm_model_cv_broad.best_score_*-1))\n",
    "print('Simple GBM RMSE is: {:.4f}.'.format(gbm_model_cv.best_score_*-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
